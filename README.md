Руководство для запуска:
    1. Запустить docker-compose, затем проверить docker ps
    2. Проверить наличие топиков в брокере кафки
docker exec dms-test3-kafka-1 kafka-topics --bootstrap-server localhost:9092 --list
Если их нет, то создать вручную:
```
docker exec dms-test3-kafka-1 kafka-topics --bootstrap-server localhost:9092 --create --topic output-decisions --partitions 1 -
-replication-factor 1
```
```
docker exec dms-test3-kafka-1 kafka-topics --bootstrap-server localhost:9092 --create --topic input-events --partitions 1 --rep
lication-factor 1
```
    3. Далее запускаем приложение, потом запускаем в разных окнах консоли продсюер и консьюмер и начинаем читать сообщения:
```
docker exec dms-test3-kafka-1 kafka-console-consumer --bootstrap-server localhost:9092 --topic output-decisions --from-beginning
```

```
docker exec -i dms-test3-kafka-1 kafka-console-producer --broker-list localhost:9092 --topic input-events
```
    4. Если приложение не запустилось, указать еще edit config -> add VM options:
```
--add-opens=java.base/java.lang=ALL-UNNAMED
--add-opens=java.base/java.nio=ALL-UNNAMED
--add-opens=java.base/java.io=ALL-UNNAMED
--add-opens=java.base/java.util=ALL-UNNAMED
--add-opens=java.base/java.util.concurrent=ALL-UNNAMED
--add-opens=java.base/sun.misc=ALL-UNNAMED
```
Сообщения:
{"eventId": "event-123", "amount": 500.50, "country": "DE"}
{"eventId": "event-456", "amount": 12000.00, "country": "US"}


    5. Если все норм то пока не знаю чо дальше делать
Предположительный план:
Что можно делать дальше (для курсовой):

Более сложные правила: Добавить больше входов, выходов, условий в DMN-таблицу. 
Попробовать другие Hit Policies (например, COLLECT, чтобы собрать несколько результатов).

Обработка ошибок: Что делать, если JSON невалидный? 
Если в DMN не сработало ни одно правило (если Hit Policy не FIRST или нет правила по умолчанию)? 
Можно отправлять такие "плохие" сообщения в отдельный топик ошибок.

Обогащение данных: Перед вызовом DMN, возможно, понадобится обогатить событие данными из другого источника 
(например, из базы данных по ID пользователя). Flink позволяет это делать.

Состояние: Использовать Flink для подсчета статистики по решениям 
(например, сколько раз было принято решение "High" за последний час).

Управление правилами: Сделать простой REST API на Spring Boot для загрузки новой версии DMN-файла 
без перезапуска всего приложения (потребуется механизм обновления DMN-определения в RichMapFunction).

Тестирование: Написать unit-тесты для DmnEvaluationMapFunction и 
интеграционные тесты для всего потока.

Документация: Подробно описать архитектуру, взаимодействие компонентов, примеры использования.

    План усовершенствования:
    
    Интеграция с PostgreSQL:

    Зачем: Абсолютно верное решение. Хранение результатов (DecisionResult) необходимо для:
    Аудита принятых решений.
    Исторического анализа. 
    Возможно, для последующего обучения моделей ML (если бы задача развивалась в эту сторону).
    
    Отчетности.
    Что еще хранить: Можно подумать о хранении:
    Самих входных событий (InputEvent) в отдельной таблице или вместе с результатом.
    Метаданных выполнения: Время обработки, какой конкретно rule ID сработал в DMN (если нужно), идентификатор версии DMN-правила.
    Статусов обработки: Если бы были ошибки, их тоже можно логировать в БД.
    Как: Интеграция Spring Data JPA с PostgreSQL – стандартный и правильный путь для Spring Boot приложения. 
    Мы можем добавить еще один Sink в Flink-пайплайн, который будет писать в базу данных, или создать отдельный Kafka Consumer (не Flink), 
    который будет слушать output-decisions и писать в БД. Для курсовой, возможно, проще добавить DB Sink во Flink.
    
    Более сложная логика в Camunda DMN:
    Зачем: Необходимо для демонстрации возможностей DMN и приближения к реальным задачам.
    Как: Можно добавить:
    Больше входных параметров (например, тип клиента, история транзакций, время суток).
    Более сложные типы данных (даты, списки).
    Использовать FEEL (Friendly Enough Expression Language) для более сложных условий в ячейках таблицы.
    Разбить логику на несколько таблиц решений, связанных через Decision Requirements Diagram (DRD).
    Использовать другие Hit Policies (e.g., COLLECT для сбора нескольких результатов).
    
    Генераторы сообщений:
    Зачем: Критически важно для тестирования и демонстрации. Ручной ввод быстро надоест и не позволит проверить
    систему под нагрузкой или с разнообразными данными.
    Как: Создать отдельный Spring компонент (например, @Service с методом, запускаемым через CommandLineRunner или 
    по расписанию @Scheduled, или даже простой REST-контроллер). Этот компонент будет использовать KafkaTemplate из 
    Spring Kafka для отправки сгенерированных InputEvent в топик input-events. Логика генерации может быть разной: 
    случайные данные, предопределенные сценарии, данные по шаблону.
    
    Сбор статистики и REST API:
    Зачем: Добавляет "операционную" ценность. Позволяет мониторить здоровье и производительность системы.
    Как:
    Flink Metrics: Flink предоставляет богатую систему метрик (количество обработанных записей, задержки, использование ресурсов). 
    Их можно экспортировать в системы мониторинга (Prometheus, Grafana).
    Spring Boot Actuator: Предоставляет стандартные эндпоинты (/actuator/metrics, /actuator/health) для базовых метрик приложения.
    Кастомные метрики: Можно реализовать подсчет специфичных для бизнеса вещей (например, сколько HIGH/MEDIUM/LOW решений принято) 
    внутри Flink-оператора (используя Accumulators или Metrics) и затем либо экспортировать их через Flink reporters, либо создать 
    свой REST-эндпоинт в Spring Boot, который будет как-то получать эти данные (возможно, через промежуточное хранилище или через REST API самого Flink).
    
    Логирование:
    Зачем: Основа для отладки и понимания работы системы.
    Как: Вы уже используете SLF4j, что хорошо. Нужно пройтись по коду и добавить осмысленные логи на разных уровнях 
    (INFO, DEBUG, WARN, ERROR) в ключевых точках: получение сообщения из Kafka, начало/конец обработки DMN, отправка результата,
    запись в БД, ошибки.
    
    Юнит и Интеграционные тесты:
    Зачем: Гарантия качества кода, упрощение рефакторинга, предотвращение регрессий. Очень важный пункт для любой серьезной разработки.
    Как:
    Юнит-тесты (JUnit, Mockito): Тестирование отдельных классов/методов в изоляции. Например, можно протестировать 
    DmnEvaluationMapFunction отдельно, подсунув ему тестовые строки JSON и проверив результат, возможно, замокав DMN Engine, 
    если нужно тестировать только логику парсинга/обертки.
    Интеграционные тесты (Spring Boot Test, Testcontainers): Тестирование взаимодействия компонентов. 
    Можно поднять в Docker (с помощью Testcontainers) Kafka и Postgres, запустить Flink-джобу в тестовом режиме (или как часть Spring Boot приложения)
    и проверить весь пайплайн: отправили сообщение в Kafka -> оно обработалось -> результат появился в выходном топике Kafka / в тестовой БД.

    Интеграция с PostgreSQL: Добавим сохранение результатов.


    Порядок:
    Усложнение логики DMN: Сделаем правила интереснее.
    Генератор сообщений: Автоматизируем входные данные.
    Улучшение логирования: Сделаем систему "прозрачнее".
    Юнит-тесты: Покроем тестами ключевые компоненты.
    Сбор статистики и REST API: Добавим мониторинг.
    Интеграционные тесты: Проверим все вместе.